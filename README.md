# Grafana Alloy Config Demo

This project serves as a reference implementation for processing and parsing structured logs using Grafana Alloy with modular pipeline configurations. It demonstrates a complete observability stack including log ingestion, "fail-first" parsing validation, label enrichment, and metric generation.

## Architecture

The stack consists of four main components orchestrated via Docker Compose:

* **Grafana Alloy**: Responsible for log collection (file watch), regex-based parsing, and forwarding data.
* **Loki**: Log aggregation and storage.
* **Prometheus**: Time-series database for metrics generated by Alloy.
* **Grafana**: Visualization and querying layer.

## Project Structure

The project follows a modular directory structure to separate pipeline logic from infrastructure configuration:

```text
.
├── alloy
│   ├── config.alloy            # Entry point for Alloy configuration
│   └── modules
│       ├── failing.alloy       # Logic for failing job simulation
│       └── demo.alloy          # Main parsing logic and metric generation
├── compose.yml                 # Service orchestration definition
├── extra
│   ├── grafana
│   │   ├── config              # The datasources config lives here
│   │   ├── data                # Persistent storage for Grafana
│   │   └── demo_dashboard.json # A local copy of the dashboard
│   ├── logs
│   │   └── trans.log           # Target log file generated by the script
│   ├── loki
│   │   ├── config              # Minimal config for loki to work
│   │   └── data                # Persistent storage for Loki
│   └── prometheus
│       ├── config              # Minimal config for prometheus to work
│       └── data                # Persistent storage for Prometheus
├── log_generator.py            # Synthetic log generator script
├── README.md
└── setup.sh                    # Environment initialization script

```

## Pipeline Philosophy

The Alloy configuration demonstrates a **modular architecture**. Instead of a monolithic configuration file, the pipeline logic is split into reusable components:

1. **Fail-First Validation**: All logs are initially tagged with `parse_status="failed"`.


2. **Multi-Format Parsing**: The pipeline attempts to parse logs using multiple regex patterns to handle inconsistent log formats (e.g., standard vs. wrapped pipes).


3. **Conditional Success**: Only when specific fields (like `method`) are successfully extracted is the label updated to `parse_status="success"`.


4. **Metric Generation**: Prometheus metrics (histograms and counters) are generated directly from the log stream within the Alloy pipeline.

## Prerequisites

* Docker Engine and Docker Compose
* Python 3 (required for the log generator)
* Git

## Installation and Usage

Follow these steps to deploy the stack and begin processing data.

### 1. Clone the Repository

```bash
git clone https://github.com/m0r4a/alloy-config-demo.git
cd alloy-config-demo

```

### 2. Infrastructure Setup

A setup script is provided to initialize the required directory structure inside `extra/` and apply the correct ownership permissions for the container volumes (Grafana, Loki, and Prometheus).

**Note:** This script requires elevated privileges (`sudo`) because it sets specific user IDs (UID 472, 10001, 65534) required by the container images.

```bash
sudo ./setup.sh
```

### 3. Start the Stack

Deploy the services using Docker Compose. This will start Alloy, Loki, Prometheus, and Grafana.

```bash
docker compose up -d
```

### 4. Generate Log Data

Start the Python log generator to create synthetic transaction logs. This script generates logs in two different formats to test the pipeline's parsing adaptability.

```bash
./log_generator.py
```

## Visualization

### Importing the Dashboard

A comprehensive dashboard is available to visualize the pipeline performance and business metrics.

1. Access Grafana at `http://localhost:3000`.
2. Enter de default credentials `admin:admin`.
3. Navigate to **Dashboards** > **New** > **Import**.
4. Enter the Dashboard ID: **24612**.
5. Click **Load**.
6. Select the **Loki** and **Prometheus** data sources if prompted (these are automatically provisioned by the stack).

### Dashboard Features

The dashboard visualizes:

* **Pipeline Health**: Real-time tracking of parsing success vs. failure rates.
* **Business Metrics**: Transaction latency histograms and volume analysis based on the `transaction_latency` metric.
* **Log Inspection**: Filterable log streams based on extracted labels.

## Disclaimer

**Do not use this configuration for production environments.**

This project is intended solely for educational and demonstration purposes. It lacks necessary security hardening, high availability, and performance optimizations. Specifically:

* Services run without authentication.
* TLS/SSL encryption is disabled.
* Resource limits are not strictly enforced.
* Data persistence is local and not backed up.
